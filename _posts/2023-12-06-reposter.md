---
layout: post
title: A reposter for your favorite social media posts
date: 2023-12-06 22:51
tags:
- mastodon
- python
---
Have you ever had bad reach with certain posts on social media? You ever been unsure of when to repost certain posts to get the best reach. Or maybe you have had trouble remember to share those posts again? Here is the solution. What if we automated the whole process. We take one time where we tend to get the most reach (or several) and we post those posts again or more exactly we share or bump them depending on what we can do.

The script we are using here is using the files from [Making a simple RSS to Mastodon poster powered by GitHub hooks](https://ellietheyeen.github.io/2023/10/29/Making-a-simple-RSS-to-Mastodon-poster-powered-by-GitHub-hooks.html) but which was modified later to use CSV and 
[Mastodon comment section on GitHub pages](https://ellietheyeen.github.io/2023/11/14/github-pages-mastodon-comments.html) is also using the `posts.csv` that are are going to use.

The below code will use a list of the posts from `posts.csv` and repost them at times on Mastodon. The first is skipped since it is an introduction posts and the 5 last written are also skipped since they are likely too recent to be posted. The 5 last posted by the script itself is also skipped in order to ensure a better rotation of posts.

`reposter.py`
```py
#!/usr/bin/python
import datetime
import mastodon
import pathlib
import random
import csv


basepath = pathlib.Path(__file__).parent # Get our current path
postfile = basepath / ".." / "rss" / "posts.csv" # The RSS posters csv file mastid,blogid
lastfile = basepath / "lastposts.csv" # This scripts file to avoid reposting blogpostmastid,repostid,date

posts = [] # List of posts

with open(postfile) as f: # Read posts file as CSV
    for i, a in enumerate(csv.reader(f, dialect="unix", quoting=csv.QUOTE_MINIMAL)): # Use the good CSV format
        if i == 0 or not a or len(a) < 2: # Skip first since header and also bad lines
            continue
        posts.append(a)

# Skip the first since introduction and also the 5 newest to not be spammy
candidates = posts[1:-5]

our_rts = [] # List of reposts

with open(lastfile) as f:
    for i, a in enumerate(csv.reader(f, dialect="unix", quoting=csv.QUOTE_MINIMAL)):
        if not a or len(a) < 2: # Skip bad lines
            continue
        our_rts.append(a[0]) # Only use the mastodon blog post ids

last_rts = our_rts[max(len(our_rts) - 5, 0):] # The last 5 regardless of size
candidates = [c for c in candidates if c[0] not in last_rts] # Filder out bad candidates

target = random.choice(candidates) # Chose a random candidate of posts
tid, t_name = target # assign to the name and id of post

access_token = "[REDACTED]" # The access token
api_base_url = "https://example.com" # Your instance
mast = mastodon.Mastodon( # Init Mastodon
    mastodon_version="1.4.3",
    access_token=access_token,
    api_base_url=api_base_url,
)
print(tid)
post = mast.status_reblog(id=tid, visibility="public") # Boost the post
print(post.id)

postdate = datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S") # Create readable date for diagnosis

with open(lastfile, "a") as f: # File open as append mode to only append lines at end and nothing else
    w = csv.writer(f, dialect="unix", quoting=csv.QUOTE_MINIMAL) # Write the good CSV format
    w.writerow([tid, post.id, postdate]) # Write the row so we know we have posted
```

Now you only have to set this up in the same way and with a cronjob and you are able to get more reach automatically for your written articles
